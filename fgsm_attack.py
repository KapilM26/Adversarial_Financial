import torch
import torch.nn.functional as F
import numpy as np
from datasets import load_dataset
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
import random
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from torch.nn.utils.rnn import pad_sequence
import torch.nn.functional as F
from datasets import load_dataset
from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset
from train_GRU import BidirectionalGRUClassifier, loss_function



def preprocess_rosbank_data(df):
    """
    Preprocess the Rosbank dataset: Normalize numerical features, one-hot encode categorical ones, and 
    group transactions into sequences (e.g., for each user or session).
    
    Args:
        df (pd.DataFrame): The raw transaction data.
        
    Returns:
        X_train_tensor (torch.Tensor): Processed features as PyTorch tensor.
        y_train_tensor (torch.Tensor): Target labels as PyTorch tensor.
    """
    # Handle missing data if any
    df = df.dropna(subset=['amount', 'MCC', 'target_flag'])  # Drop rows with missing values in important columns

    label_encoder = LabelEncoder()
    df['MCC'] = label_encoder.fit_transform(df['MCC']) 

    # Normalize continuous features like 'amount'
    scaler = StandardScaler()
    df['amount'] = scaler.fit_transform(df[['amount']]) 

    # Group transactions by user (assuming ⁠ cl_id⁠  as user ID) and treat each user's transactions as a sequence
    grouped = df.groupby('cl_id')

    # Convert the features and target to lists of tensors (for sequence input to RNN)
    X_sequences = []
    y_sequences = []
    for _, group in grouped:
        # Convert the group (user's transactions) to a tensor of features (scaled amount, encoded MCC)
        X_sequences.append(torch.tensor(group[['amount', 'MCC']].values, dtype=torch.float32))  
        
        # We take the last target flag of the group for classification
        y_sequences.append(torch.tensor(group["target_flag"].values[-1], dtype=torch.long)) 
    
    # Pad sequences to ensure uniform length across the batch
    padded_X_sequences = pad_sequence(X_sequences, batch_first=True, padding_value=0)  # Padding with zeros
    padded_y_sequences = torch.tensor(y_sequences, dtype=torch.long)

    return padded_X_sequences, padded_y_sequences


# FGSM Attack Function
def fgsm_attack(model, X, y, epsilon=1.0, n_steps=30):
    """
    Perform the Fast Gradient Sign Method (FGSM) attack with multiple steps.
    
    Args:
        model: The trained model.
        X: Input features (batch of sequences).
        y: True labels.
        epsilon: Step size (perturbation magnitude).
        n_steps: Number of steps for the iterative attack.
    
    Returns:
        perturbed_X: The adversarially perturbed inputs.
    """
    # Set requires_grad attribute of input to compute gradients
    X.requires_grad = True
    
    # Start the attack with the original input
    perturbed_X = X.clone()
    
    for step in range(n_steps):
        # Forward pass the data through the model
        output = model(perturbed_X)
        
        # Compute the loss
        loss = F.cross_entropy(output, y)
        
        # Backpropagate the loss to get the gradients of the input
        model.zero_grad()
        loss.backward()
        
        # Get the sign of the gradients
        sign_data_grad = perturbed_X.grad.data.sign()
        
        # Apply the perturbation
        perturbed_X = perturbed_X + epsilon * sign_data_grad
        
        # Clamp the perturbed input to ensure it's within a valid range
        perturbed_X = torch.clamp(perturbed_X, 0, 1)
    
    return perturbed_X



# Evaluate the model under attack
def evaluate_model_under_attack(model, test_loader, epsilon=0.1):
    """
    Evaluate the model on the test set using adversarial examples generated by FGSM.
    
    Args:
        model: The trained model.
        test_loader: The DataLoader for the test set.
        epsilon: Perturbation magnitude for FGSM.
    
    Returns:
        accuracy: The accuracy of the model on the adversarial examples.
        loss: The loss of the model on the adversarial examples.
    """
    model.eval()  # Set the model to evaluation mode
    correct_predictions = 0
    total = 0
    test_loss = 0
    

    for batch_idx, (sequences, labels) in enumerate(test_loader):
        # Generate adversarial examples using FGSM
        perturbed_sequences = fgsm_attack(model, sequences, labels, epsilon)
        
        # Forward pass on the perturbed data
        output = model(perturbed_sequences)
        loss = loss_function(output, labels)
        
        test_loss += loss.item()  # Accumulate loss
        
        # Get predictions
        _, predicted = torch.max(output, 1)
        correct_predictions += (predicted == labels).sum().item()
        total += labels.size(0)
    
    accuracy = correct_predictions / total  # Accuracy on the adversarial examples
    avg_loss = test_loss / len(test_loader)  # Average loss on the adversarial examples
    
    return accuracy, avg_loss


train_dataset = load_dataset("dllllb/rosbank-churn", "train")
train_data = train_dataset['train']
df_train = pd.DataFrame(train_data)

# Simple 65/35 split as requested
train_df, test_df = train_test_split(df_train, test_size=0.35, random_state=42)

# Preprocess each dataset
X_test_tensor, y_test_tensor = preprocess_rosbank_data(test_df)

# Create DataLoaders for batched training
batch_size = 1024
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
# Now, let's use the `evaluate_model_under_attack` function on the test data:
epsilon = 1  # You can adjust the perturbation magnitude
model = torch.load('best_GRU_epoch_11.pth', weights_only=False)

attack_accuracy, attack_loss = evaluate_model_under_attack(model, test_loader, epsilon)
print(f"Adversarial Test Accuracy: {attack_accuracy*100:.2f}%")
print(f"Adversarial Test Loss: {attack_loss:.4f}")
